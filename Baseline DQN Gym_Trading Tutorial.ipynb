{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:32:06.956003Z",
     "start_time": "2018-02-12T05:32:06.459831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! explorer ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Gym_Trading](https://github.com/myao9494/gym_trading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:51:23.259628Z",
     "start_time": "2018-02-12T05:51:23.244618Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gym_trading\n",
    "import gym\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、csvファイルのアドレスを指定します。\n",
    "\n",
    "-----------------\n",
    "First, define the address for the CSV data\n",
    "\n",
    "-----------------\n",
    "csvファイルの仕様は、  \n",
    "Open\tHigh\tLow\tClose  \n",
    "と思われる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:51:24.510522Z",
     "start_time": "2018-02-12T05:51:24.506445Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = \"data/EURUSD60.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym環境を作成する（カスタマイズされた環境）\n",
    "\n",
    "initialise_simulator（）はenv.make（ 'trading-v0'）の後に呼び出さなければなりません。  \n",
    "この関数内では、次の引数を指定します\n",
    "\n",
    "* **csv_name**: Address of the data\n",
    "\n",
    "\n",
    "* **trade_period**: (int), 取引毎の最大期間（最大で、どのぐらいの期間ホールドするか？）Max of duration of each trades. *Default: 1000*\n",
    "\n",
    "\n",
    "* **train_split**: (float), トレーニング用のデータの割合（残りはテスト用）Percentage of data set for training. *Default: 0.7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:51:25.845567Z",
     "start_time": "2018-02-12T05:51:25.584439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-12 14:51:25,586] Making new env: trading-v0\n",
      "C:\\Users\\mineo\\Anaconda3\\envs\\mine36\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:106: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Return       ATR  Open Trade  Duration Trade\n",
      "Date_Time                                                          \n",
      "2013-12-02 02:00:00  0.421251  0.355142         0.0             0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('trading-v0')\n",
    "env.initialise_simulator(csv, trade_period=20, train_split=0.7)#初期はtrade_period＝50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States map\n",
    "states_mapは、時間刻みの半分間隔（分足なら30秒）の特徴の極値で囲まれた、離散化された観測空間です。  \n",
    "また、取引期間とアクティブな取引のブール値を使用します。この観測値は、アルゴリズムが実行されている間の微分です。 \n",
    "\n",
    "---------------\n",
    "\n",
    "states_map is a discretized observation space bounded by the extreme values of features with an interval of 0.5. Also I use trade duration and boolean of active trade. This observations are dinamical, while the algorithm runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:51:26.674564Z",
     "start_time": "2018-02-12T05:51:26.664057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42125144,  0.35514246,  0.        ,  0.        ],\n",
       "       [ 1.7094119 ,  0.45689009,  0.        ,  0.        ],\n",
       "       [-0.59190109,  0.45925152,  0.        ,  0.        ],\n",
       "       ..., \n",
       "       [-1.00049976, -0.45976391,  0.        ,  0.        ],\n",
       "       [-0.20124835, -0.51999441,  0.        ,  0.        ],\n",
       "       [ 0.31455146, -0.61636501,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.sim.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The magic (Deep Q-Network)\n",
    "\n",
    "Baselines OpenAIのポイントは、強化学習アルゴリズムの高品質な実装です。強化取引のための多くのプロジェクトは、独自の実装を使用しているため、小さなバグやメンテナンス/改善が困難です。\n",
    "\n",
    "このトピックを掘り下げるには、多くの優れたリソースがあります。しかし、Q_learningとDQNの核心は次の図で表現することができます\n",
    "\n",
    "-------------------------\n",
    "\n",
    "The point of Baselines OpenAI is set of high-quality implementations of reinforcement learning algorithms. A lot of projects for reinforcement trading uses their own implementations, causing small bugs or hard to maintenance/improvement.\n",
    "\n",
    "There are a lot of good resources to drill down on this topic. \n",
    "But well above, the core of Q_learning and DQNs can express with the next diagrams\n",
    "\n",
    "![Image of DQN](https://raw.githubusercontent.com/udacity/deep-learning/master/reinforcement/assets/deep-q-learning.png)\n",
    "\n",
    "\n",
    "![Image of Q-Network](https://raw.githubusercontent.com/udacity/deep-learning/master/reinforcement/assets/q-network.png)\n",
    "\n",
    "\n",
    "### Learning resources:\n",
    "\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "\n",
    "http://minpy.readthedocs.io/en/latest/tutorial/rl_policy_gradient_tutorial/rl_policy_gradient.html\n",
    "\n",
    "http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html\n",
    "\n",
    "http://kvfrans.com/simple-algoritms-for-solving-cartpole/\n",
    "\n",
    "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149\n",
    "\n",
    "https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model \n",
    "なので、さっそくやってみよう！まずは、ネットワークをセットします。  \n",
    "【ネットワーク層の説明】  \n",
    "out = layers.fully_connected(層のインプット, num_outputs：アウトプットの数, activation_fn=tf.nn.tanh（活性化関数：ここでは双曲線関数、ちなみにデフォルト値はReLU関数）)  \n",
    "\n",
    "--------------\n",
    "So, let's get our hands dirty. First set our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:51:28.292150Z",
     "start_time": "2018-02-12T05:51:28.272638Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "        out = layers.fully_connected(out, num_outputs=128, activation_fn=tf.nn.tanh)\n",
    "        out = layers.fully_connected(out, num_outputs=64, activation_fn=tf.nn.tanh)\n",
    "        out = layers.fully_connected(out, num_outputs=32, activation_fn=tf.nn.tanh)\n",
    "        out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、すべてのエピソードの最後に使用するrun_test関数を定義する\n",
    "\n",
    "------------\n",
    "And define run_test function to use in the end of every episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:51:29.629643Z",
     "start_time": "2018-02-12T05:51:29.583613Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_test(env, act, episodes=1, final_test=False):\n",
    "    obs = env._reset(train=False)\n",
    "    start = env.sim.train_end_index + 1\n",
    "    end = env.sim.count - 2\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        while done is False:\n",
    "            action = act(obs[None])\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if not final_test:\n",
    "            journal = pd.DataFrame(env.portfolio.journal)\n",
    "            profit = journal[\"Profit\"].sum()\n",
    "            return env.portfolio.average_profit_per_trade, profit\n",
    "        else:\n",
    "            print(\"Training period  %s - %s\" % (env.sim.date_time[start], env.sim.date_time[end]))\n",
    "            print(\"Average Reward is %s\" % (env.portfolio.average_profit_per_trade))\n",
    "\n",
    "    if final_test:\n",
    "        env._generate_summary_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the enviroment!\n",
    "\n",
    "この時点で、私たちは環境を立ち上げてエピソードを実行することができます。 最も重要なのは次のとおりです。\n",
    "\n",
    "* 私たちが望む報酬でepisode_rewardsを設定します。たとえば、各取引を最大限にしたい場合：episode_rewards [-1] + = rew\n",
    "* 解決された関数を設定します。 関数の結果が真になると、トレーニングは停止します。 \n",
    "    例：is_solved = np.mean（episode_rewards [-101：-1]）> 1000またはt == 100000\n",
    "* deepq.build_trainをインスタンス化します。 それがOpenAIのベースラインの中核です。\n",
    "\n",
    "----------------\n",
    "\n",
    "At this point, we can start up the enviroment and run the episodes. The most important is:\n",
    "\n",
    "* Set the `episode_rewards` with the reward that we want. For example if we want maximice each trade: `episode_rewards[-1] += rew`\n",
    "* Set the solved function. The training will stop when the outcome of function get True. For example: `is_solved = np.mean(episode_rewards[-101:-1]) > 1000 or t == 100000`\n",
    "* Instanciate `deepq.build_train`. That is the core of Baseline of OpenAI.\n",
    "\n",
    "### `build_train` Creates the train function:\n",
    "\n",
    "#### パラメータ\n",
    "* make_obs_ph：str - > tf.placeholderまたはTfInput - >名前を取り、その名前の入力のプレースホルダを作成する関数\n",
    "\n",
    "* q_func：（tf.Variable、int、str、bool） - > tf.Variable - >次の入力を受け取るモデル：\n",
    "    1. observation_in：オブジェクト - >観測プレースホルダの出力\n",
    "    1. num_actions：int - >アクション数\n",
    "    1. scope：str\n",
    "    1. reuse：bool - >は外部変数scopeに渡され、すべてのアクションの値とともにテンソルの形状（batch_size、num_actions）を返します。\n",
    "    \n",
    "    \n",
    "* num_actions：int - >アクション数\n",
    "\n",
    "* reuse：bool - >グラフ変数を再利用するかどうか\n",
    "\n",
    "* optimizer：tf.train.Optimizer - >オプティマイザをQ-learningの目的に使用します。\n",
    "\n",
    "* grad_norm_clipping：floatまたはNone->この勾配ノルムをクリップします。 Noneの場合、クリッピングは実行されません。\n",
    "\n",
    "* gamma：float - >割引率。\n",
    "\n",
    "* double_q：bool - > trueの場合はDouble Q Learning（https://arxiv.org/abs/1509.06461)を使用します。一般に、有効にすることをお勧めします。\n",
    "\n",
    "* scope：strまたはVariableScope - > variable_scopeのオプションスコープ。\n",
    "\n",
    "* reuse：boolまたはNone - >変数を再利用する必要があるかどうか。スコープを再利用できるようにする必要があります。\n",
    "\n",
    "#### 戻り値\n",
    "* act：（tf.Variable、bool、float） - > tf.Variable - >与えられた観測を選択して処理する関数。\n",
    "\n",
    "* train：（オブジェクト、np.array、np.array、オブジェクト、np.array、np.array） - > np.array - > Bellmanの式の誤差を最適化します。詳細はファイルの先頭を参照してください。\n",
    "\n",
    "* update_target：（） - >（） - >最適化されたQ関数からターゲットQ関数にパラメータをコピーします。デバッグ：{str：function} - > q_valuesのようなデバッグデータを出力する一連の関数。\n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "* make_obs_ph: str -> tf.placeholder or TfInput -> a function that takes a name and creates a placeholder of input with that name\n",
    "\n",
    "* q_func: (tf.Variable, int, str, bool) -> tf.Variable -> the model that takes the following inputs:\n",
    "    * observation_in: object -> the output of observation placeholder\n",
    "    * num_actions: int -> number of actions\n",
    "    * scope: str\n",
    "    * reuse: bool -> should be passed to outer variable scope and returns a tensor of shape (batch_size, num_actions) with values of every action.\n",
    "\n",
    "* num_actions: int -> number of actions\n",
    "\n",
    "* reuse: bool -> whether or not to reuse the graph variables\n",
    "\n",
    "* optimizer: tf.train.Optimizer -> optimizer to use for the Q-learning objective.\n",
    "\n",
    "* grad_norm_clipping: float or None -> clip gradient norms to this value. If None no clipping is performed.\n",
    "\n",
    "* gamma: float -> discount rate.\n",
    "\n",
    "* double_q: bool -> if true will use Double Q Learning (https://arxiv.org/abs/1509.06461).In general it is a good idea to keep it enabled.\n",
    "\n",
    "* scope: str or VariableScope -> optional scope for variable_scope.\n",
    "\n",
    "* reuse: bool or None -> whether or not the variables should be reused. To be able to reuse the scope must be given.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "* act: (tf.Variable, bool, float) -> tf.Variable -> function to select and action given observation.\n",
    "\n",
    "* train: (object, np.array, np.array, object, np.array, np.array) -> np.array  -> optimize the error in Bellman's equation.See the top of the file for details.\n",
    "\n",
    "* update_target: () -> () -> copy the parameters from optimized Q function to the target Q function.\n",
    "debug: {str: function} -> a bunch of functions to print debug data like q_values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:54:29.908639Z",
     "start_time": "2018-02-12T05:51:31.294596Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mineo\\Anaconda3\\envs\\mine36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\mineo\\Anaconda3\\envs\\mine36\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "steps                     | 2138\n",
      "episodes                  | 1\n",
      "% time spent exploring    | 79\n",
      "--\n",
      "mean episode reward       | nan\n",
      "Total operations          | 398\n",
      "Avg duration trades       | 3.14\n",
      "Total profit              | -1066.0\n",
      "Avg profit per trade      | -2.172\n",
      "--\n",
      "Total profit test:        > -112.0\n",
      "Avg profit per trade test > 0.113\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 4277\n",
      "episodes                  | 2\n",
      "% time spent exploring    | 58\n",
      "--\n",
      "mean episode reward       | -2594.2\n",
      "Total operations          | 422\n",
      "Avg duration trades       | 2.31\n",
      "Total profit              | -2042.0\n",
      "Avg profit per trade      | -3.693\n",
      "--\n",
      "Total profit test:        > -462.3\n",
      "Avg profit per trade test > -3.501\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 6416\n",
      "episodes                  | 3\n",
      "% time spent exploring    | 37\n",
      "--\n",
      "mean episode reward       | -2784.9\n",
      "Total operations          | 362\n",
      "Avg duration trades       | 1.86\n",
      "Total profit              | -986.0\n",
      "Avg profit per trade      | -2.416\n",
      "--\n",
      "Total profit test:        > -313.1\n",
      "Avg profit per trade test > -3.352\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 8555\n",
      "episodes                  | 4\n",
      "% time spent exploring    | 16\n",
      "--\n",
      "mean episode reward       | -2640.2\n",
      "Total operations          | 247\n",
      "Avg duration trades       | 1.47\n",
      "Total profit              | -619.0\n",
      "Avg profit per trade      | -2.762\n",
      "--\n",
      "Total profit test:        > -338.9\n",
      "Avg profit per trade test > -3.953\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 10694\n",
      "episodes                  | 5\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -2216.7\n",
      "Total operations          | 86\n",
      "Avg duration trades       | 2.1\n",
      "Total profit              | -289.0\n",
      "Avg profit per trade      | -2.667\n",
      "--\n",
      "Total profit test:        > 24.9\n",
      "Avg profit per trade test > -0.893\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 12833\n",
      "episodes                  | 6\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -1809.8\n",
      "Total operations          | 37\n",
      "Avg duration trades       | 2.24\n",
      "Total profit              | 53.0\n",
      "Avg profit per trade      | -0.432\n",
      "--\n",
      "Total profit test:        > 4.4\n",
      "Avg profit per trade test > -3.071\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 14972\n",
      "episodes                  | 7\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -1480.6\n",
      "Total operations          | 38\n",
      "Avg duration trades       | 3.11\n",
      "Total profit              | -115.0\n",
      "Avg profit per trade      | -1.968\n",
      "--\n",
      "Total profit test:        > -47.2\n",
      "Avg profit per trade test > -3.858\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 17111\n",
      "episodes                  | 8\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -1259.2\n",
      "Total operations          | 33\n",
      "Avg duration trades       | 1.39\n",
      "Total profit              | 38.0\n",
      "Avg profit per trade      | -0.782\n",
      "--\n",
      "Total profit test:        > -30.8\n",
      "Avg profit per trade test > -1.843\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 19250\n",
      "episodes                  | 9\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -1051.2\n",
      "Total operations          | 34\n",
      "Avg duration trades       | 1.53\n",
      "Total profit              | -152.0\n",
      "Avg profit per trade      | -2.526\n",
      "--\n",
      "Total profit test:        > -2.9\n",
      "Avg profit per trade test > -3.2\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 21389\n",
      "episodes                  | 10\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -902.0\n",
      "Total operations          | 21\n",
      "Avg duration trades       | 1.76\n",
      "Total profit              | 74.0\n",
      "Avg profit per trade      | 0.919\n",
      "--\n",
      "Total profit test:        > -53.2\n",
      "Avg profit per trade test > -2.657\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 23528\n",
      "episodes                  | 11\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -761.4\n",
      "Total operations          | 27\n",
      "Avg duration trades       | 2.11\n",
      "Total profit              | 82.0\n",
      "Avg profit per trade      | 2.193\n",
      "--\n",
      "Total profit test:        > -48.3\n",
      "Avg profit per trade test > -3.881\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 25667\n",
      "episodes                  | 12\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -636.1\n",
      "Total operations          | 37\n",
      "Avg duration trades       | 2.78\n",
      "Total profit              | -124.0\n",
      "Avg profit per trade      | -2.784\n",
      "--\n",
      "Total profit test:        > -29.3\n",
      "Avg profit per trade test > -5.89\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 27806\n",
      "episodes                  | 13\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -524.8\n",
      "Total operations          | 38\n",
      "Avg duration trades       | 3.95\n",
      "Total profit              | 25.0\n",
      "Avg profit per trade      | 0.495\n",
      "--\n",
      "Total profit test:        > -31.6\n",
      "Avg profit per trade test > -6.143\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 29945\n",
      "episodes                  | 14\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -437.9\n",
      "Total operations          | 24\n",
      "Avg duration trades       | 2.5\n",
      "Total profit              | -36.0\n",
      "Avg profit per trade      | 0.858\n",
      "--\n",
      "Total profit test:        > -6.9\n",
      "Avg profit per trade test > -2.7\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 32084\n",
      "episodes                  | 15\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -390.4\n",
      "Total operations          | 32\n",
      "Avg duration trades       | 4.84\n",
      "Total profit              | 60.0\n",
      "Avg profit per trade      | 2.047\n",
      "--\n",
      "Total profit test:        > -82.6\n",
      "Avg profit per trade test > -7.225\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 34223\n",
      "episodes                  | 16\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -302.2\n",
      "Total operations          | 36\n",
      "Avg duration trades       | 2.03\n",
      "Total profit              | 0.0\n",
      "Avg profit per trade      | -1.075\n",
      "--\n",
      "Total profit test:        > -234.3\n",
      "Avg profit per trade test > -4.33\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 36362\n",
      "episodes                  | 17\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -257.8\n",
      "Total operations          | 22\n",
      "Avg duration trades       | 3.45\n",
      "Total profit              | -62.0\n",
      "Avg profit per trade      | 1.777\n",
      "--\n",
      "Total profit test:        > -17.7\n",
      "Avg profit per trade test > -1.756\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 38501\n",
      "episodes                  | 18\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -198.4\n",
      "Total operations          | 46\n",
      "Avg duration trades       | 4.98\n",
      "Total profit              | -71.0\n",
      "Avg profit per trade      | -0.939\n",
      "--\n",
      "Total profit test:        > -18.7\n",
      "Avg profit per trade test > -1.308\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 40640\n",
      "episodes                  | 19\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -137.9\n",
      "Total operations          | 31\n",
      "Avg duration trades       | 4.1\n",
      "Total profit              | 52.0\n",
      "Avg profit per trade      | 2.31\n",
      "--\n",
      "Total profit test:        > -147.4\n",
      "Avg profit per trade test > -3.895\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 42779\n",
      "episodes                  | 20\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -62.2\n",
      "Total operations          | 45\n",
      "Avg duration trades       | 6.53\n",
      "Total profit              | -88.0\n",
      "Avg profit per trade      | 0.518\n",
      "--\n",
      "Total profit test:        > -85.9\n",
      "Avg profit per trade test > -3.976\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "steps                     | 44918\n",
      "episodes                  | 21\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -82.0\n",
      "Total operations          | 47\n",
      "Avg duration trades       | 4.34\n",
      "Total profit              | 41.0\n",
      "Avg profit per trade      | 0.791\n",
      "--\n",
      "Total profit test:        > -111.5\n",
      "Avg profit per trade test > -3.742\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 47057\n",
      "episodes                  | 22\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -80.6\n",
      "Total operations          | 50\n",
      "Avg duration trades       | 3.38\n",
      "Total profit              | 113.0\n",
      "Avg profit per trade      | 2.042\n",
      "--\n",
      "Total profit test:        > -132.4\n",
      "Avg profit per trade test > -2.884\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 49196\n",
      "episodes                  | 23\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | -7.0\n",
      "Total operations          | 49\n",
      "Avg duration trades       | 3.49\n",
      "Total profit              | 84.0\n",
      "Avg profit per trade      | 1.316\n",
      "--\n",
      "Total profit test:        > -110.3\n",
      "Avg profit per trade test > -3.095\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "steps                     | 51335\n",
      "episodes                  | 24\n",
      "% time spent exploring    | 2\n",
      "--\n",
      "mean episode reward       | 69.3\n",
      "Total ope"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10126 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with U.make_session(8):\n",
    "    \n",
    "    act, train, update_target, debug = deepq.build_train(\n",
    "        make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "        q_func=model,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(50000)\n",
    "    # Create the schedule for exploration starting from 1 (every action is random) down to\n",
    "    # 0.02 (98% of actions are selected according to values predicted by the model).\n",
    "    exploration = LinearSchedule(schedule_timesteps=10000, initial_p=1.0, final_p=0.02)\n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    update_target()\n",
    "\n",
    "    episode_rewards = [0.0]\n",
    "    obs = env.reset()\n",
    "    l_mean_episode_reward = []\n",
    "    for t in itertools.count():\n",
    "        # Take action and update exploration to the newest value\n",
    "        action = act(obs[None], update_eps=exploration.value(t))[0]\n",
    "\n",
    "        new_obs, rew, done, _ = env.step(action)\n",
    "\n",
    "        # Store transition in the replay buffer.\n",
    "        replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "\n",
    "        obs = new_obs\n",
    "\n",
    "        episode_rewards[-1] += rew\n",
    "\n",
    "        is_solved = np.mean(episode_rewards[-101:-1]) > 500 or t >= 300000\n",
    "        is_solved = is_solved and len(env.portfolio.journal) > 2\n",
    "        \n",
    "        if done:\n",
    "\n",
    "            journal = pd.DataFrame(env.portfolio.journal)\n",
    "            profit = journal[\"Profit\"].sum()\n",
    "\n",
    "            try:\n",
    "                print(\"-------------------------------------\")\n",
    "                print(\"steps                     | {:}\".format(t))\n",
    "                print(\"episodes                  | {}\".format(len(episode_rewards)))\n",
    "                print(\"% time spent exploring    | {}\".format(int(100 * exploration.value(t))))\n",
    "\n",
    "                print(\"--\")\n",
    "                l_mean_episode_reward.append(round(np.mean(episode_rewards[-101:-1]), 1))\n",
    "\n",
    "                print(\"mean episode reward       | {:}\".format(l_mean_episode_reward[-1]))\n",
    "                print(\"Total operations          | {}\".format(len(env.portfolio.journal)))\n",
    "                print(\"Avg duration trades       | {}\".format(round(journal[\"Trade Duration\"].mean(), 2)))\n",
    "                print(\"Total profit              | {}\".format(round(profit), 1))\n",
    "                print(\"Avg profit per trade      | {}\".format(round(env.portfolio.average_profit_per_trade, 3)))\n",
    "\n",
    "                print(\"--\")\n",
    "\n",
    "                reward_test, profit = run_test(env=env, act=act)\n",
    "                print(\"Total profit test:        > {}\".format(round(profit, 2)))\n",
    "                print(\"Avg profit per trade test > {}\".format(round(reward_test, 3)))\n",
    "                print(\"-------------------------------------\")\n",
    "            except Exception as e:\n",
    "                print(\"Exception: \", e)\n",
    "                # Update target network periodically.\n",
    "\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(0)\n",
    "\n",
    "\n",
    "\n",
    "        if is_solved:\n",
    "            # Show off the result\n",
    "            env._generate_summary_stats()\n",
    "            run_test(env, act, final_test=True)\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "            if t > 500:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(32)\n",
    "                train(obses_t, actions, rewards, obses_tp1, dones, np.ones_like(rewards))\n",
    "            if t % 500 == 0:\n",
    "                update_target()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:54:30.110250Z",
     "start_time": "2018-02-12T05:54:29.952170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VOXZ//HPlQRCWEPYIYmsioCs\nUbDuSysurRu4PLal2hZptcvT/vpUq09b+9TWpa1dtFpqbbFVAcWtdUFR3FpBwiL7EtYEAgFCSCAk\nZLl+f8zBppqEQCY5M8n3/XrNa2buc2bmyij55tz3fe5j7o6IiEg0JYRdgIiItDwKFxERiTqFi4iI\nRJ3CRUREok7hIiIiUadwERGRqFO4iIhI1ClcREQk6hQuIiISdUlhFxCW7t27e//+/cMuQ0Qkrixe\nvHiPu/c42n6tNlz69+9PdnZ22GWIiMQVM9vakP3ULSYiIlGncBERkahTuIiISNTFbLiY2RYzW2Fm\ny8wsO2hLM7PXzWxDcN81aDcz+62Z5ZjZcjMbG271IiKtW8yGS+A8dx/t7lnB89uAN9x9CPBG8Bzg\nYmBIcJsKPNzslYqIyEdiPVw+7nJgRvB4BnBFjfbHPWIBkGpmfcIoUEREYjtcHHjNzBab2dSgrZe7\n5wME9z2D9n5Abo3X5gVtIiISglg+z+UMd99hZj2B181sbT37Wi1tn7h+cxBSUwEyMzOjU6WIyDE4\nUF5JftEhduwvY+f+Q1RUOW2TEkhOSiA5KTG4TwjaEmmblECXlDb07tKuUZ/r7izNLeIfH+Zz+yVD\naZPYtMcWMRsu7r4juC8ws+eA04BdZtbH3fODbq+CYPc8IKPGy9OBHbW853RgOkBWVtYnwkdEJBoO\nlFfy+uqd5BVGQiR//yHyi8rYsf8QJWWVx/Wew/p05rJRfbjslL5kdmvf4NftKi7j2SXbeWZxLht3\nH6RdmwSuGtuPEf26HFcdDRWT4WJmHYAEdy8JHn8G+AnwIjAFuCe4fyF4yYvArWY2ExgP7D/SfSYi\n0pwWbSnkO7OXkVt4CIBuHdrSJ7Udmd3aM2FgGn1SU+jTpR19U1Po3bkdyW0SKK+o5nBVNeUV1ZRX\nVnG4spryyuqP7rcXlfLyip3c9+o67nt1HaPSu3DZyL5cOrIPfVNTPlFDeWUV81YX8MziXN5ev5tq\nh6wTunLv1QO55JQ+dGrXpsm/B3OPvT/gzWwg8FzwNAl40t3vNrNuwGwgE9gGTHb3QjMz4EFgIlAK\n3Oju9a7tkpWV5Vr+RUSipbyyil+9vp7p72wio2t77rnqFMae0JV2bRKj9hm5haW8vCKffyzPZ8X2\n/QCMzUz9KGgKist5ZnEuL3y4g6LSCnp3bsfV4/oxaVwGA7p3iEoNZra4xgzeuveLxXBpDgoXEYmW\ntTuL+fbMZazdWcL1p2Vw56XD6JDctB1DW/Yc5KUV+fz9wx2s3VnyUXvbpAQuGt6bSePSOXNwdxIT\nahuSPn4Kl6NQuIhIY1VVO396bxO/mLuezilJ3Hv1SC44uVez15FTcIC5q3bSJaUNnx3Zly7tm67b\nq6HhEpNjLiIisS63sJTvPv0hH2wu5KLhvfjZlafQrWNyKLUM7tmRwT0Hh/LZdVG4iIgcA3fnmcV5\n3PX31QD8YvIorh7bj8jQrxyhcBERCZQermTvgcMUlVZQWHqYotLDFB48zL7SCvYdPMy+0sPk7jvE\nh7lFnDYgjV9OHkVGWsOnBbcmChcRafVWbt/Pr+etZ96aglq3m0GXlDaktW9Lavs23Hnpydx4xoCo\nD5a3JAoXEWm11uQX8+t565m7ahed2yXxtXMHMaBbB1LbtyGtQ1u6dmhL1/Zt6ZLSRkFyjBQuItLq\nbNhVwq/nbeClFfl0Sk7i2xcO4aYzB9C5GU4ubC0ULiLSamzafYDfvLGBFz/cQfs2iXzj/MF85cyB\nTTp1t7VSuIhIi+bubNx9kIff2shzS/NITkrk5rMHMfXsgaR1aBt2eS2WwkVEWhR3Z+veUhZu3suC\nTYUs3LSXHfvLSE5K4KYzBjDt3EF0D+l8lNZE4SIicc3d2bznIAs3F7Jg014WbipkZ3EZAN07tmX8\nwG58bUAaFw3vTc/OjVu2XhpO4SIicSd//yH+mbOX9zbs5l8b91JQUg5Aj07JjB+QxoSB3ZgwsBuD\nenTQyY0hUbiISMwrLqtg4aZC3tuwm/dy9rBx90Egspz9pwZ35/SB3Rg/MI2B3RUmsULhIiIxaeX2\n/by2aifv5ezhw7z9VFU7KW0SOW1AGtedmsmZQ7pzUq9OJOj8k5ikcBGRmHKgvJJ7XlnD3xZsI8Fg\nZHoqXztnEGcO6c6YzFSSk6J3fRRpOgoXEYkZ76zfze3PrmDH/kPcdMYAvnXBEJ2DEqcULiISuuKy\nCu7+xxpmZecysEcHnpl2OuNOSAu7LGkEhYuIhOrNtbv4wbMrKSgpY9o5g/j2hUOiemlgCYfCRURC\nUVR6mJ/8fTXPLt3OSb068YcvjGNURmrYZUmUKFxEpNm9unIndz6/kqLSw3zz/MHccv5gDdS3MAoX\nEWk2Czft5cH5Oby7YQ/D+nRmxk2nMrxvl7DLkibQYsLFzCYCvwESgUfd/Z6QSxIRIsuzvJezh9+9\nkcMHWwrp3rEtd1xyMl86oz9tEhPCLk+aSIsIFzNLBB4CPg3kAYvM7EV3Xx1uZSKtl7vz5toCfvdm\nDstyi+jduR0/+uwwrj8tUwP2rUCLCBfgNCDH3TcBmNlM4HJA4SLSzKqrnbmrdvK7N3NYnV9MetcU\n7r5yBJPGpWtcpRVpKeHSD8it8TwPGB9SLSKtUmVVNS+tyOeh+Tms33WAAd07cP+kkVwxpp+6v1qh\nlhIutS0u5J/YyWwqMBUgMzOzqWsSaRXKKqp4ZnEe09/ZxLbCUk7s1ZHfXDeay0b21XXnW7GWEi55\nQEaN5+nAjo/v5O7TgekAWVlZnwgfEWm44rIK/rZgK4+9t4U9B8oZnZHKnZeezIUn99JiktJiwmUR\nMMTMBgDbgeuA/wq3JJHwVVZVkxTlLqmCkjL+/M8t/O39rZSUV3L2iT342jmDmDAwTcvdy0daRLi4\ne6WZ3QrMJTIV+TF3XxVyWSKhcHfeWreb3725gSXbiujZKZn0rilkpLUnvWsK6V3bk9E18rhvagpt\nkxoWPlv3HmT6O5t4enEelVXVXHJKH6adM4gR/XSeinxSiwgXAHd/GXg57DpEwnJkltaD83NYtaOY\nfqkpTDtnEIUHy8ktPMSSbfv4x/J8qqr/3SNsBr06taNdmwSqHaqqHXen2qHaPbhFHhcfqiApIYGr\nx6Vz89kD6d+9Q4g/rcS6FhMuIq1VZVU1f1++g4fmbySnoP5ZWpVV1ewsLiNv3yFyC0vJ23eI7UWH\nqKiqJsEsuBG5T/j348QEI61DW647NUPXoZcGUbiIxKnyyiqeXbKdh9/ayLbCUk7q1YnfXj+GS0/p\nU+csraTEBNK7tie9a3smDOzWzBVLa6JwEYkz+0sreGZJHo++u4n8/WWMTO/CnZeO0ywtiSkKF5E4\nUF3t/GvjXmZl5zJ31U4OV1Zzav+u3Hv1SM4a0l2ztCTmKFxEYtj2okM8k53H04tzydt3iM7tkrj+\n1AwmZ2VolpbENIWLSIwpr6xi3uoCZmXn8u6G3bjDGYO78b2LTuKi4b216KPEBYWLSIxYtWM/T2fn\n8cKy7ewrraBPl3Z847zBTM7KICOtfdjliRwThYtIiPYdPMzzy7bzdHYeq/OLaZuYwKeH9eKaUzM4\nc3B3rc0lcUvhItLMKquqeWfDbp7OzmPeml1UVDkj+nXmrs8N53Oj+tK1Q9uwSxRpNIWLSDNwd3IK\nDjBnyXaeXZJHQUk5aR3a8oUJ/Zmclc7JfTqHXaJIVClcRJrIwfJK3t+4l7fWF/D2+t3kFh4iMcE4\n76QeTBqXwflDezZ4XS+ReKNwEYkSd2dDwQHeWhcJk0Wb93G4qpr2bRP51KBuTD1rIBeN6E3PTlo+\nRVo+hYtII32YW8TMRdt4e91uduwvA+CkXp340hn9OffEHozr31WX95VWR+Eicpy2Fx3i/lfX8vyy\nHXRMTuLMwd355gU9OPvEHvRNTQm7PJFQKVxEjtGB8koeeWsjf3x3EwC3njeYaecOomOy/jmJHKF/\nDSINVFXtPJ2dyy9eW8+eA+VcMbov35s4lH46ShH5BIWLSAO8t2EPP31pNWt3lpB1QlcenZLF6IzU\nsMsSiVkKF5F6bNhVws9fWcubawvISEvh9zeM5eIRvbUKschRKFxEaqiudlbu2M+81buYt6aA1fnF\ndEpO4vaLhzLlU/21aKRIAylcpNUrq6jinzl7mLemgDfW7KKgpJwEg3EndOW2i4cyeVw63Tomh12m\nSFxRuEirVFxWwSsr8pm3poB3N+ymrKKaDm0TOeekHlwwtBfnDe1Jmtb4EjluChdpVQ5XVvPEwq38\n5o0NFJVW0LdLO67JyuCCk3sxYWCaTnYUiZKYCxcz+zHwVWB30PQDd3852HY78GWgCvimu88N2icC\nvwESgUfd/Z7mrltim7szd9VO7nllLVv2lnLG4G589zMnMSYjVYPzIk0g5sIl8IC7/6Jmg5kNA64D\nhgN9gXlmdmKw+SHg00AesMjMXnT31c1ZsMSupdv2cfdLa8jeuo8hPTvy5xtP5dwTeyhURJpQrIZL\nbS4HZrp7ObDZzHKA04JtOe6+CcDMZgb7KlxauW17S7lv7lr+sTyfHp2S+flVpzB5XDpJiVqJWKSp\nxWq43GpmXwSyge+6+z6gH7Cgxj55QRtA7sfax9f2pmY2FZgKkJmZGe2aJUbsL63gd29uYMb7W0hK\nSOBbFwxh6tkD6aDlWUSaTSj/2sxsHtC7lk13AA8D/wd4cP9L4Cagtj4MB2r7M9Rr+1x3nw5MB8jK\nyqp1H4lvr6zI5wfPraDoUAXXjMvgO585kV6dtcS9SHMLJVzc/cKG7GdmfwT+ETzNAzJqbE4HdgSP\n62qXVuJgeSV3/X0Vs7PzGJnehSevHqmrO4qEKOb6Ccysj7vnB0+vBFYGj18EnjSzXxEZ0B8CfEDk\niGaImQ0AthMZ9P+v5q1awrQst4hvz1zK1sJSbj1vMN+6cAhtNK4iEqqYCxfgPjMbTaRrawtwM4C7\nrzKz2UQG6iuBW9y9CsDMbgXmEpmK/Ji7rwqjcGleVdXOw2/l8MC8DfTu3I6ZX53A+IHdwi5LRABz\nb51DD1lZWZ6dnR12GXKc8vaV8t+zlrFoyz4+O6ovP71iBF1S2oRdlkiLZ2aL3T3raPvF4pGLSL1e\nWLadO59biQMPXDuKK0b30zkrIjFG4SJxo7isgh8+v5Lnl+1g3Ald+fW1o8lIax92WSJSC4WLxIUN\nu0qY+tfFbCss5TufPpGvnztIJ0OKxLA6w8XMllLH+SIA7j62SSoS+ZhXV+bz3dkfktI2iae+OoHT\nBqSFXZKIHEV9Ry6TgvtpRGZh/TV4fgNQ0pRFiUBkNtgDr6/nwfk5jM5I5ZHPj6N3F50QKRIP6gwX\nd98IYGafcvczamxaamb/BO5q6uKk9dp/qIJvz1zK/HW7uTYrg59cMVzL4YvEkYaMuXQ0swnuvgDA\nzMYDHZu2LGnN1u8qYerj2WwvOsRPrxjBDeMzNRtMJM40JFy+DPzFzNoRGYMpI7LWl0jUfXx8Jau/\nxldE4lG94WJmicAJ7j7CzLoBuPveZqlMWhWNr4i0LPWGi7tXmdm3gTkKFWkqB8or+caTS5i/bjfX\nnZrBXZdrfEUk3jWkW2xuEDCzgINHGt29uMmqklajoKSMG/+8iLU7SzS+ItKCNCRcbg7uv1ujzQFd\nbUsaZfOeg3zxsYXsKTnMo1OyOO+knmGXJCJRctRwcfeMo+0jcqyW5RZx018WAfDU1AmMzkgNuSIR\niaYGLf9iZkOBYcBHI6zu/mRTFSUt2/x1BXz9b0vo3qktj980ngHdO4RdkohE2VHDxczuBD4DDCVy\nzZSLgPcAhYscs6ezc7nt2RUM7d2JP994Kj07aUaYSEvUkJX/rgXOA/Ld/QvAKLTgpRwjd+eh+Tl8\n75nlnD6wG7NuPl3BItKCNSQkDgVTkivNrBOwExjYxHVJC1JV7dz191U8/v5WLh/dl/snjaJtklY0\nFmnJGhIuS80sFXgMyAaKgSVNWpW0GGUVVfz3rGW8snInU88eyG0Th5KQoKnGIi1dQ2aLHZmK/JCZ\nzQU6u7vCRY7qYHklX5mRzfub9nLnpSfzlbN0wCvSWjRkQP8x4F3gXXfPafqSpCXYX1rBl/7yAcvz\n9vPAtaO4ckx62CWJSDNqSMf3TGAA8EczyzGzWWZ2SxPXJXFs74Fyrv/jAlZtL+ah/xqrYBFphY4a\nLu7+GvBj4HvAH4HTgf9uzIea2WQzW2Vm1WaW9bFttwchts7MLqrRPjFoyzGz22q0DzCzhWa2IQi+\nto2pTRpn5/4yrvnD+2zac4A/Tsli4ojeYZckIiE4argE4yz/AqYAm4EJ7j64kZ+7ErgKeOdjnzUM\nuA4YDkwEfm9micHqzA8BFxM5mfP6YF+Ae4EH3H0IsI/IJQIkBLmFpUz+w7/YVVzOjBtP45wTe4Rd\nkoiEpCHdYuuBSmAIcCIw2MySG/Oh7r7G3dfVsulyYKa7l7v7ZiAHOC245bj7Jnc/TKSr7nKLrHB4\nPvBM8PoZwBWNqU2OT07BASY/8j7Fhyp54ivjGT+wW9gliUiIGjJb7BsAZtYF+CLwV6AnkNIE9fQD\nFtR4nhe0AeR+rH080A0ocvfKWvaXZrImv5jPP7oQM5g5dQIn9+kcdkkiErKGzBabBpwFnArkA48T\nmT12tNfNA2rrcL/D3V+o62W1tDm1H2F5PfvXVdNUYCpAZqYWdY6GZblFTHnsA9q3TeSJr4xnYA9d\nAVtEGnYSZVfg98CioEuqQdz9wuOoJw+ouQpzOrAjeFxb+x4g1cySgqOXmvvXVtN0YDpAVlZWnSEk\nDbNg016+/JdFdO+UzN++PJ6MtPZhlyQiMaIhs8V+DlQRGWjHzNLMrKn+7H8RuM7Mks1sAJFxng+A\nRcCQYGZY26CWF93dgfnApOD1U4C6jookilZu38+X/vwBfVNTmH3z6QoWEfkPDZktdifwI+DOoCmF\nRq6IbGZXmlkekWnNLwUz0nD3VcBsYDXwKnCLu1cFRyW3ElmVeQ0wO9gX4PvAd8wsh8gYzJ8aU5sc\nXXFZBbc8uYTUlLY8NXUCvTprAUoR+U8W+eO/nh3MlgFjgCXuPiZoW+7uI5uhviaTlZXl2dnZYZcR\nd9ydW55cwtxVu5g1dQJZ/dPCLklEmpGZLXb3rKPt15CpyOVB95MHb6z+j1bs8fe38vKKnXzvopMU\nLCJSp4aEy7Nm9hDQxcxuBF4D/ty0ZUksWp5XxE9fWs35Q3syVYtQikg9GnKey71mdjFwmMiFwu52\n91eavDKJKfsPRcZZenRM5peTR2nZfBGpV4OuKBmEySsAFnGtu89q0sokZrg7//PMh+QXlTHr5tPp\n2kHLt4lI/ersFjOzjmb2PTP7tZmdH4TKNGAjkTP1pZX48z+3MHfVLr4/cSjjTugadjkiEgfqO3L5\nK3AQeB+4hciqyJ2Aa9xd06xaiWW5Rfz8lTVceHIvvnLWgLDLEZE4UV+4DHb3UwDM7BEiZ8Of4O7F\nzVKZhK6o9DC3PLGEnp3a8cvJo4isEyoicnT1zRarOPLA3auAzQqW1sPd+X9PL6egpIyHbhhLl/Zt\nwi5JROJIfUcuo8ysMHhsQKfguQHu7jrJoQV79N3NzFuzix9eNozRGalhlyMicaa+cNGUoFZq8dZ9\n3PvqWiYO782NZ/QPuxwRiUN1hkvQFSatTHFZBd98ail9Uttx76SRGmcRkePSoPNcpPX40Qur2Flc\nxuybT6dLisZZROT4NGT5F2klXvxwB88t3c6t5w3W+Swi0igKFwFge9Eh7nhuBWMyU/nG+YPDLkdE\n4lyd3WJmto/aLxms2WItTFW189+zllFd7fzm2jEkJepvDhFpnPrGXLo3WxUSqj+8s5EPNhdy/6SR\nZHbTFRVEpPEaPFvMzNKAmpccrPNa9RI/VuTt51evreeSU3ozaVx62OWISAvRkMscX2pm64E8YGFw\n/2ZTFyZN79DhKr41ayndOybzsytP0bRjEYmahnSu3w2cAaxz9wzgIuCtpixKmsdPX1rN5j0H+dU1\no0htr3NmRSR6GhIule6+G0gwM3P314GxTVyXNLHXV+/iiYXb+OpZA/nUYA2viUh0NeQkyv1m1gF4\nD3jczAqA6qYtS5pSQUkZ35+znGF9OvPdz5wYdjki0gI15MjlCqAM+DaR7rDtwGWN+VAzm2xmq8ys\n2syyarT3N7NDZrYsuD1SY9s4M1thZjlm9lsLBgjMLM3MXjezDcG9zv6rh7vzvaeXc7C8kt9cN5rk\npMSwSxKRFqgh4XK7u1e5e4W7/8ndfwV8p5GfuxK4Cninlm0b3X10cJtWo/1hYCowJLhNDNpvA95w\n9yHAG8FzqcPj72/l7fW7uePSkxnSq1PY5YhIC9WQcJlYS9uljflQd1/j7usaur+Z9QE6u/v77u7A\n40SOqAAuB2YEj2fUaJePyS0s5Wcvr+G8k3rwhQknhF2OiLRgdYaLmd1sZkuBk8xsSY3bBmB1E9Y0\nwMyWmtnbZnZW0NaPyBToI/KCNoBe7p4PENz3bMLa4tr0dzZR7c7dmnYsIk2svgH92US6mX7Of3Y1\nlbh7wdHe2MzmAb1r2XSHu79Qx8vygUx332tm44DnzWw4kSVnPq62pWmOVtNUIl1rZGZmHuvL41pB\nSRmzsnO5emw6fVNTwi5HRFq4+s7Q3wfsAyab2QjgzGDTu8BRw8XdLzzWYty9HCgPHi82s43AiUSO\nVGqePp7Ov1cI2GVmfdw9P+g+q7M2d58OTAfIyso65nCKZ4+9t4XKqmpuPmdQ2KWISCvQkDP0byFy\nFJMZ3Gab2debohgz62FmicHjgUQG7jcF3V0lZjYhmCX2ReDI0c+LwJTg8ZQa7RLYX1rB3xZs5dKR\nfRnQvUPY5YhIK9CQ81xuBk5z9wMAZvYz4F/A74/3Q83sSuB3QA/gJTNb5u4XAWcDPzGzSqAKmObu\nhcHLvgb8BUgBXgluAPcQCbwvA9uAycdbV0v1+PtbOFBeydfP1VGLiDSPhoSLARU1nldQ+xhIg7n7\nc8BztbTPAebU8ZpsYEQt7XuBCxpTT0tWeriSx/65mfOH9uTkPp3DLkdEWon6rueS5O6VwF+BBWZ2\n5Jf+lfx76q/EuKc+yGVfaQW3nKejFhFpPvUduXwAjHX3+8xsPnAWkSOWae6+qFmqk0Ypr6zij+9s\nYvyANMadoGu7iUjzqS9cPur6CsJEgRJnnluynZ3FZdw3aWTYpYhIK1NfuPQwszqXeQmWgZEYVVlV\nzcNvb+SUfl04a4hWPRaR5lVfuCQCHWnk4L2E4+WVO9m6t5RHPj9WZ+OLSLOrL1zy3f0nzVaJRI27\n8/v5OQzq0YHPDKttkQQRkaZV30mU+nM3Ts1fV8DanSV8/dzBJCToP6OINL/6wkXnjsQhd+fBN3Po\nl5rC50b3DbscEWml6gyXGmfGSxxZuLmQJduKmHbOQNokNuSKCiIi0affPi3MQ/Nz6N4xmclZGWGX\nIiKtmMKlBVmeV8S7G/bwlbMG0K6NLl8sIuFRuLQgv5+/kc7tkrhhfOu6Vo2IxB6FSwuxYVcJr67a\nyZRP9adTuzZhlyMirZzCpYWY/s4mUtokcuMZA8IuRURE4dISlJRV8I/l+Vwxpi9pHdqGXY6IiMKl\nJfjH8nwOVVRxjWaIiUiMULi0ALMW5XJir46MzkgNuxQREUDhEvfW7yphWW4R12RlaIFKEYkZCpc4\nN3tRLkkJxpVj+oVdiojIRxQucexwZTXPLd3OhSf3olvH5LDLERH5iMIljr25dhd7Dx7m2lM1kC8i\nsUXhEsdmLcqld+d2nH1ij7BLERH5D6GEi5ndb2ZrzWy5mT1nZqk1tt1uZjlmts7MLqrRPjFoyzGz\n22q0DzCzhWa2wcxmmVmrONFj5/4y3l6/m6vH9SNR12wRkRgT1pHL68AIdx8JrAduBzCzYcB1wHBg\nIvB7M0s0s0TgIeBiYBhwfbAvwL3AA+4+BNgHfLlZf5KQzFmSR7XD5HHqEhOR2BNKuLj7a+5eGTxd\nAKQHjy8HZrp7ubtvBnKA04JbjrtvcvfDwEzgcovMvT0feCZ4/Qzgiub6OcJSXe3Mzs5l/IA0+nfv\nEHY5IiKfEAtjLjcBrwSP+wG5NbblBW11tXcDimoE1ZH2WpnZVDPLNrPs3bt3R6n85vfBlkK27i3V\nQL6IxKykpnpjM5sH9K5l0x3u/kKwzx1AJfDEkZfVsr9Tewh6PfvXyt2nA9MBsrKy6twv1s1elEun\n5CQuHtEn7FJERGrVZOHi7hfWt93MpgCXARe4+5Ff9HlAzT/H04EdwePa2vcAqWaWFBy91Ny/RSou\nq+DllflcNTadlLa6IJiIxKawZotNBL4PfM7dS2tsehG4zsySzWwAMAT4AFgEDAlmhrUlMuj/YhBK\n84FJweunAC80188Rhr9/uIOyimqu1SKVIhLDmuzI5SgeBJKB14P1sBa4+zR3X2Vms4HVRLrLbnH3\nKgAzuxWYCyQCj7n7quC9vg/MNLOfAkuBPzXvj9K8Zi/KZWjvToxM7xJ2KSIidQolXNx9cD3b7gbu\nrqX9ZeDlWto3EZlN1uKt3VnMh3n7+d/LhmmRShGJabEwW0waaPaiPNokapFKEYl9Cpc4EVmkMo9P\nD+ulq02KSMxTuMSJeWt2sa+0QlebFJG4oHCJE7MW5dKnSzvOGqJFKkUk9ilc4sCOokO8s2E3k8al\na5FKEYkLCpc4MGdxHq5FKkUkjihcYlx1tTN7cS6fGtSNzG7twy5HRKRBFC4xbuHmQnILD2kgX0Ti\nisIlxs1ZkkfH5CQuGl7bGqAiIrFJ4RLDSg9X8sqKfC45pbcWqRSRuKJwiWFzV+3k4OEqrh6bfvSd\nRURiiMIlhs1ZvJ2MtBRO7Z8WdikiIsdE4RKjdhQd4p8b93DVmHQSdG6LiMQZhUuMen7ZdtzhqrFa\npFJE4o/CJQa5O3MW53Fq/64d2TKaAAAK9klEQVSc0K1D2OWIiBwzhUsM+jBvPxt3H9RAvojELYVL\nDJqzOI/kpAQuGdkn7FJERI6LwiXGlFdW8fflO/jM8N50btcm7HJERI6LwiXGzF9bQFFpBVdrIF9E\n4pjCJcY8s3g7PTsl67otIhLXFC4xZO+Bct5aV8CVY/rpui0iEtdCCRczu9/M1prZcjN7zsxSg/b+\nZnbIzJYFt0dqvGacma0wsxwz+62ZWdCeZmavm9mG4L5rGD9TNLz44Q4qq52rNEtMROJcWEcurwMj\n3H0ksB64vca2je4+OrhNq9H+MDAVGBLcJgbttwFvuPsQ4I3geVyasySPEf06c1LvTmGXIiLSKKGE\ni7u/5u6VwdMFQL1/qptZH6Czu7/v7g48DlwRbL4cmBE8nlGjPa6s21nCyu3FOrdFRFqEWBhzuQl4\npcbzAWa21MzeNrOzgrZ+QF6NffKCNoBe7p4PENz3bOqCm8KzS/JISjA+N6pv2KWIiDRaUlO9sZnN\nA2q7wtUd7v5CsM8dQCXwRLAtH8h0971mNg543syGA7WNbvtx1DSVSNcamZmZx/ryJlNZVc1zS7dz\n7kk96dYxOexyREQarcnCxd0vrG+7mU0BLgMuCLq6cPdyoDx4vNjMNgInEjlSqdlflA7sCB7vMrM+\n7p4fdJ8V1FPTdGA6QFZW1jGHU1N5L2cPBSXlTBqnc1tEpGUIa7bYROD7wOfcvbRGew8zSwweDyQy\ncL8p6O4qMbMJwSyxLwIvBC97EZgSPJ5Soz1uzFmyndT2bThvaFz26ImIfEKTHbkcxYNAMvB6MKN4\nQTAz7GzgJ2ZWCVQB09y9MHjN14C/AClExmiOjNPcA8w2sy8D24DJzfVDRENxWQWvrdrJNVkZJCfp\nUsYi0jKEEi7uPriO9jnAnDq2ZQMjamnfC1wQ1QLrsXTbPvYcOMynh/WKyvu9vDyf8spqrh6nWWIi\n0nLEwmyxuOHu3PfqOr751FJW7dgflfecsySPQT06MCq9S1TeT0QkFihcjoGZ8ZvrR5Pavg1TH1/M\nngPljXq/rXsPsmjLPq4el07QPSgi0iIoXI5Rz07tmP6FLPYeLOdrf1vM4crq436vpz7IxQyuHKNZ\nYiLSsihcjsMp6V24f9IoFm3Zxw9fWEkwk/qYPPruJh55eyOXntKHPl1SmqBKEZHwhDVbLO59dlRf\n1u0s4cH5OQzt3YkvnTGgQa9zd37x2joemr+RS07pzS+vGdXElYqIND+FSyN859Mnsm5XCf/30hoG\n9+zEmUO617t/VbXzvy+s5MmF27j+tEx+esUILa0vIi2SusUaISHBeODa0Qzu0ZFbnlzC5j0H69y3\nvLKKbz61lCcXbuPr5w7iZ1cqWESk5VK4NFLH5CQenZJFgsFXH8+muKziE/scLK/kKzOyeWlFPndc\ncjL/M3GoZoeJSIumcImCjLT2/P6GcWzZc5BvPbWUqup/D/DvO3iYGx5dyL827uX+SSP56tkDQ6xU\nRKR5KFyi5PRB3fjx54Yzf91u7pu7FoCd+8u45g/vszq/mIdvGMvkrIyQqxQRaR4a0I+iz084gXU7\nS/jD25vo3K4NTy7cxv5DFcy48TROH9Qt7PJERJqNwiXKfvjZYWwoKOH+uevo1qEtM6dOYEQ/Le0i\nIq2LwiXK2iQm8PAN4/jdmzl8fkImA3t0DLskEZFmp3BpAl07tOWHnx0WdhkiIqHRgL6IiESdwkVE\nRKJO4SIiIlGncBERkahTuIiISNQpXEREJOoULiIiEnUKFxERiTo7nkv0tgRmthvYWsum7sCeZi4n\nWuK5dojv+uO5dojv+uO5doi/+k9w9x5H26nVhktdzCzb3bPCruN4xHPtEN/1x3PtEN/1x3PtEP/1\n10XdYiIiEnUKFxERiTqFyydND7uARojn2iG+64/n2iG+64/n2iH+66+VxlxERCTqdOQiIiJRp3AJ\nmNlEM1tnZjlmdlvY9RwrM9tiZivMbJmZZYddT33M7DEzKzCzlTXa0szsdTPbENx3DbPG+tRR/4/N\nbHvw/S8zs0vCrLEuZpZhZvPNbI2ZrTKzbwXtcfH911N/zH//ZtbOzD4wsw+D2u8K2geY2cLgu59l\nZm3DrjUa1C0GmFkisB74NJAHLAKud/fVoRZ2DMxsC5Dl7jE/X97MzgYOAI+7+4ig7T6g0N3vCcK9\nq7t/P8w661JH/T8GDrj7L8Ks7WjMrA/Qx92XmFknYDFwBfAl4uD7r6f+a4jx79/MDOjg7gfMrA3w\nHvAt4DvAs+4+08weAT5094fDrDUadOQScRqQ4+6b3P0wMBO4POSaWix3fwco/Fjz5cCM4PEMIr8w\nYlId9ccFd8939yXB4xJgDdCPOPn+66k/5nnEgeBpm+DmwPnAM0F7zH73x0rhEtEPyK3xPI84+R+2\nBgdeM7PFZjY17GKOQy93z4fILxCgZ8j1HI9bzWx50G0Wk91KNZlZf2AMsJA4/P4/Vj/EwfdvZolm\ntgwoAF4HNgJF7l4Z7BKPv3tqpXCJsFra4q2/8Ax3HwtcDNwSdN1I83kYGASMBvKBX4ZbTv3MrCMw\nB/i2uxeHXc+xqqX+uPj+3b3K3UcD6UR6TE6ubbfmrappKFwi8oCMGs/TgR0h1XJc3H1HcF8APEfk\nf9x4sivoTz/Sr14Qcj3HxN13Bb84qoE/EsPff9DfPwd4wt2fDZrj5vuvrf54+v4B3L0IeAuYAKSa\nWVKwKe5+99RF4RKxCBgSzNpoC1wHvBhyTQ1mZh2CwU3MrAPwGWBl/a+KOS8CU4LHU4AXQqzlmB35\nxRy4khj9/oNB5T8Ba9z9VzU2xcX3X1f98fD9m1kPM0sNHqcAFxIZM5oPTAp2i9nv/lhptlggmLr4\nayAReMzd7w65pAYzs4FEjlYAkoAnY7l+M3sKOJfIarC7gB8BzwOzgUxgGzDZ3WNy0LyO+s8l0iXj\nwBbg5iNjGLHEzM4E3gVWANVB8w+IjFvE/PdfT/3XE+Pfv5mNJDJgn0jkD/vZ7v6T4N/vTCANWAp8\n3t3Lw6s0OhQuIiISdeoWExGRqFO4iIhI1ClcREQk6hQuIiISdQoXERGJOoWLSJSYWVWNVXmXHW11\nbTObZmZfjMLnbjGz7o19H5Fo0lRkkSgxswPu3jGEz91CnKyILa2HjlxEmlhwZHFvcC2PD8xscND+\nYzP7f8Hjb5rZ6mDhxZlBW5qZPR+0LQhOwsPMupnZa2a21Mz+QI218czs88FnLDOzPwSXkxBpdgoX\nkehJ+Vi32LU1thW7+2nAg0RWgvi424Ax7j4SmBa03QUsDdp+ADwetP8IeM/dxxBZtiUTwMxOBq4l\nsojpaKAKuCG6P6JIwyQdfRcRaaBDwS/12jxV4/6BWrYvB54ws+eJLIUDcCZwNYC7vxkcsXQBzgau\nCtpfMrN9wf4XAOOARZEluEghhheglJZN4SLSPLyOx0dcSiQ0Pgf8r5kNp/5LQdT2HgbMcPfbG1Oo\nSDSoW0ykeVxb4/79mhvMLAHIcPf5wP8AqUBH4B2Cbi0zOxfYE1y7pGb7xcCRC2O9AUwys57BtjQz\nO6EJfyaROunIRSR6UoKrDB7xqrsfmY6cbGYLifxBd/3HXpcI/C3o8jLgAXcvMrMfA382s+VAKf9e\nEv8u4CkzWwK8TWQVY9x9tZndSeSKpAlABXALsDXaP6jI0WgqskgT01RhaY3ULSYiIlGnIxcREYk6\nHbmIiEjUKVxERCTqFC4iIhJ1ChcREYk6hYuIiESdwkVERKLu/wNAd5L21+v6qwAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27a4112c048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(l_mean_episode_reward)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It converges!\n",
    "Time to implement your own strategy :) \n",
    "\n",
    "And remember, be careful of overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
